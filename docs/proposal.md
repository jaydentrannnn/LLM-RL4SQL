---
layout: default
title: Proposal
---

## Summary of the Project
Our project uses RL to convert natural language into SQL queries. Our input is a natural language and a database schema, and our output is an executable SQL query. This is an application-driven approach, as our overall goal is to enhance a model’s ability to generate correct SQL for real-world-like datasets. We plan to compare RL fine-tuned models against baseline text-to-SQL models. Doing so, we aim to understand how different reward designs and model choices affect reasoning quality and generalization in SQL generation. 


## Project goals
- Minimum Goal
  Use GRPO on a variety of baseline reasoning models (Qwen, Deepseek, LLama) with a fixed simple binary reward. We will evaluate which models and prompts yield the best learned model.
- Realistic Goal
  Use a more robust model with larger weights to grade outputs. This will result in a more fine-tuned reward signal that is continuous on the interval [0, 1] and should provide better learning feedback in comparison to a simple binary reward.
- Moonshot Goal
  Have feedback on the specific reasoning generated by robust models. In this way instead of critiquing the final answer, the entire thought process is analyzed.



## AI/ML Algorithms
We plan to use the Group Relative Policy Optimization (GRPO) reinforcement learning algorithm with verifiable rewards.  

## Evaluation Plan
**Quantitative:** To quantitatively evaluate our text-to-SQL model, we will run tests on well-known text-to-SQL datasets like BIRD or Spider 1.0 and compare our model, which has been fine-tuned with reinforcement learning. We will use Spider 1.0 for standard testing because its data is somewhat organized, as is typical for a normal dataset, while we will use BIRD for efficiency testing since it is way more complex and contains much more dirty data, which is a harsh environment to see how well the model performs with unorganized data. Our baseline depends on which model we decide to choose as the base model to fine-tune but it would potentially be the QwenCoder-7B or QwenCoder-32B, which both have execution accuracy on the BIRD data set at around 70%. Our primary metrics will be Exact Set Match and Execution Accuracy. While an exact set match would focus on checking if the generated SQL logic matched the ground truth query, the execution accuracy evaluates the output by executing the generated SQL query on the database and comparing the result set to the ground truth result set. Besides, we might also track the result fidelity to account for queries that return partially correct data or too many results. We estimate that our reward fine-tuning could improve Execution Accuracy by approximately 5–10% over the baseline by better handling complex reasoning rewards that standard loss functions miss.  

**Qualitative:** For qualitative analysis, we will perform a sanity check by querying LLM and manual inspection. Since our moonshot goal involves generating “critiqued rewards,” we will query LLM to inspect output generated by the reward model during training while simultaneously examining some of the output ourselves to ensure that the rewards are correctly given. We will select specific “toy cases” that involve complex logic and compare the SQL generated by the baseline versus our RL-fine-tuned model to see the differences in quality of the two outputs. Besides, we will also experiment with everything with different base models and different reward models to find out which has the best performance and the reasons behind that.  

## AI Tool Usage
We did not use any AI tool during our planning process but I (Jayden) did used it to sharpen the evaluation part of the proposal by query the main points and ask the agent to formalize the answer.  